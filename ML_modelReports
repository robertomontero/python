# Check if models are accurate
# Confusion Matrix: a matrix of true positive TP (correct prediction on target type = x), 
  # false positive FP (incorrect prediction on target type = x), 
  # true negative TN (correct prediction on target type = y), 
  # false negative FN (incorrect prediction on target type = y)
# A classification report can be built using metrics of the confusion matrix
  # precision (ppv): total number of correctly labeled-x targets / total number of labeled-x targets
  # recall (sensitivity): TP / (TP+FN)
  # F1 score: 2*precision*recall / (precision+recall)
  # support: total number of considered inputs per classification
# FOR LOGISTIC REGRESSION
# roc curve: is constructed to see the accuracy of the model (Its a relationship between false positive (x) and true positive (y)). 
  # The more to the left top side of the graph the line is, the more accurate / or is the area beneath the curve is close to 1, 
  # it is a good accuracy, close to 0.5 is a very bad accuracy. Area under the curve can be calculated using LR method 
  # sklearn.metrics.roc_auc_score
# roc area under the curve

# Compute predicted probabilities: y_pred_prob
y_pred_prob = logreg.predict_proba(X_test)[:,1]

# Generate ROC curve values: fpr, tpr, thresholds
fpr, tpr, threshold = roc_curve(y_test, y_pred_prob)

# Plot ROC curve
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show().roc_auc_score

# CONFUSION MATRIX / CLASIFFICATION REPORT
# Import necessary modules
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

# Create training and test set
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.4, random_state = 42)

# Instantiate a k-NN classifier: knn
knn = KNeighborsClassifier(n_neighbors = 6)

# Fit the classifier to the training data
knn.fit(X_train,y_train)

# Predict the labels of the test data: y_pred
y_pred = knn.predict(X_test)

# Generate the confusion matrix and classification report
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# ROC CURVE
# Import necessary modules
from sklearn.metrics import roc_curve

# Compute predicted probabilities: y_pred_prob
y_pred_prob = logreg.predict_proba(X_test)[:,1]

# Generate ROC curve values: fpr, tpr, thresholds
fpr, tpr, threshold = roc_curve(y_test, y_pred_prob)

# Plot ROC curve
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()

# Area under ROC
# OPTION 1: use roc_auc_score
from sklearn.metrics import roc_auc_score
area_under_curve = roc_auc_score(y_test, y_pred_prob)

# OPTION 2: use sklearn.model_selection.cross_val_score
from sklearn.model_selection import cross_val_score
cv_auc = cross_val_score(logreg, X, y, cv=5, scoring='roc_auc')
